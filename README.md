# Big Data & Machine Learning!

This project deals with the analysis of the Unihan data set.
Unihan is an effort of the authors of the Unicode standard to standardize the so-called "Han characters", which are used in China, Taiwan, Hong Kong, Japan, Singapore and have also been used in countries like Korea and Vietnam.

## Questions

Possible questions that can be answered and visualized are the following:

- Which are the most frequently used Radicals?
- Which are the most frequently used Characters?
- Is a trend visible?
- Does the Chinese Language Exam HSK cover the most frequently used words?
- Are certain words together in a sentence?
- Which are the most commonly used words in chinese Music?
- Which are the most commonly used words in chinese News Paper?
- Which are the most commonly used words in chinese Movies?
- Are there Hapax legomenon for a certain area (words that are used only once)?

## Requirements

- Python 3.8+

## Installation

```bash
git clone https://github.com/ptrstn/technikum-big-data
git cd technikum-big-data
pip install -r requirements.txt
python main.py
```
